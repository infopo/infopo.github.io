---
layout: post
title: Spark - 17/01/26
category: acorn수업
---

## 설치

```
///ip세팅, 하둡 사용자로 로그인
$ wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz
$ tar -vxzf spark-2.1.0-bin-hadoop2.7.tgz
$ ln - spark-2.1.0-bin-hadoop2.7 spark
$ vi ~/.bashrc    ///github
$ source ~/.bashrc

$ cd spark/conf
$ cp spark-defaults.conf.template spark-defaults.conf
$ cp log4j.properties.template log4j.properties
$ cp spark-env.sh.template spark-env.sh
$ vi spark-env.sh   ///github

$ vi spark-defaults.conf    ///github
```
```
///다른 터미널을 하나 더 연다
///하둡 사용자로 로그인
$ start-all.sh
$ mr-jobhistory-daemon.sh start historyserver

$ pwd
/home/hadoop

$ mkdir practice4_spark_test
$ cd spark
$ cp README.md ../practice4_spark_test/
$ hdfs dfs -put README.md /user/hadoop

///spark-shell까지 실행하면 SparkSubmit도 생긴다
$ jps
5872 NameNode
7152 SparkSubmit
7634 JobHistoryServer
6439 ResourceManager
7672 Jps
6090 DataNode
6542 NodeManager
6270 SecondaryNameNode
```

---

## 실행

```
///spark shell로 진입
/// /home/hadoop/spark 로 들어가서 할 것
///hadoop 사용자로 로그인만 하고, 현재 경로는 상관 없음
$ spark-shell --master local[2]
    ---------
    Spark context Web UI available at http://192.168.0.91:4040
    Spark context available as 'sc' (master = local[2], app id = local-1485393893868).
    Spark session available as 'spark'.
    Welcome to
      ____ __
      / __/__ ___ _____/ /__
      _\ \/ _ \/ _ `/ __/ '_/
      /___/ .__/\_,_/_/ /_/\_\ version 2.1.0
      /_/

    Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_111)
    Type in expressions to have them evaluated.
    Type :help for more information.

    scala> val lines = sc.textFile("README.md")
    lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:24

    scala> lines.count()
    res0: Long = 104

    scala> :quit
    ---------
```
