---
layout: post
title: Spark - 17/02/03
category: acorn수업
---

### take, count, map, foreach, flatMap, collect

```
scala> val nums = sc.parallelize(List(1,2,3,4,5))
nums: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala> nums.collect
res0: Array[Int] = Array(1, 2, 3, 4, 5)

scala> nums.take(2)
res1: Array[Int] = Array(1, 2)

scala> nums.count
res2: Long = 5

scala> val even = nums.map(x => x * x)
even: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:26

scala> even
res3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:26

scala> even.collect
res4: Array[Int] = Array(1, 4, 9, 16, 25)

scala> val other = even.filter(x => x % 2 == 0)
other: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at filter at <console>:28

scala> other.collect
res5: Array[Int] = Array(4, 16)

scala> other.foreach(println)
16
4

scala> nums.flatMap(x => 0.to(x))
res7: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[3] at flatMap at <console>:27

scala> nums.flatMap(x => 0.to(x)).collect
res8: Array[Int] = Array(0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5)
```

---

### reduceByKey, groupByKey, sortByKey

```
scala> val pets = sc.parallelize(List(("cat",1), ("dog",1), ("cat",1)))
pets: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[5] at parallelize at <console>:24

scala> pets.collect
res9: Array[(String, Int)] = Array((cat,1), (dog,1), (cat,1))

scala> pets.foreach(println)
(cat,1)
(dog,1)
(cat,1)

scala> pets.reduceByKey(_+_)
res11: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at <console>:27

scala> pets.reduceByKey(_+_).collect
res12: Array[(String, Int)] = Array((dog,1), (cat,2))

scala> pets.groupByKey.collect
res13: Array[(String, Iterable[Int])] = Array((dog,CompactBuffer(1)), (cat,CompactBuffer(1, 1)))

scala> pets.sortByKey().collect
res15: Array[(String, Int)] = Array((cat,1), (cat,1), (dog,1))
```

---

### partitions, repartition

```
scala> val a = sc.parallelize(1 to 100, 3)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[12] at parallelize at <console>:24

///reduceByKey는 Map형식의 자료일 때만 사용가능
scala> a.reduce(_+_)
res16: Int = 5050

scala> var rdd = sc.parallelize(List(1,2,10,4,5,2,1,1,1),3)
rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[13] at parallelize at <console>:24

scala> rdd.collect
res18: Array[Int] = Array(1, 2, 10, 4, 5, 2, 1, 1, 1)

scala> rdd.partitions.length
res19: Int = 3

scala> var rdd2 = rdd.repartition(5)
rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[17] at repartition at <console>:26

scala> rdd2.partitions.length
res20: Int = 5
```

---

### sample

```
scala> val a = sc.parallelize(1 to 10000, 3)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[18] at parallelize at <console>:24

scala> a.sample(false, 0.1, 0).count
res21: Long = 1032

///false-중복불가 / 0.1-전체데이터 중 쓸 개수 / 0-seed값(random값들도 일정한 분포를 가짐)
///seed값은 없어도 됨
scala> a.sample(false, 0.1, 0).collect
res22: Array[Int] = Array(10, 39, 41, 53, 54, 58, 60, 80, 89, 98, 113, 114, 128, 134, 144, 161, 173, 176, 177, 198, 201, 211, 212, 213, 218, 224, 232, 235, 241, 242, 243, 246, 264, 287, 307, 319, 321, 325, 334, 339, 353, 354, 366, 367, 373, 392, 403, 407, 419, 426, 429, 434, 458, 465, 466, 478, 492, 495, 513, 520, 524, 528, 541, 558, 569, 587, 596, 604, 608, 611, 627, 631, 648, 650, 658, 662, 670, 678, 686, 707, 709, 737, 752, 753, 757, 758, 775, 787, 803, 804, 806, 825, 832, 849, 850, 852, 863, 870, 912, 915, 922, 926, 933, 947, 948, 956, 965, 985, 986, 1003, 1011, 1012, 1018, 1039, 1053, 1055, 1072, 1076, 1083, 1085, 1093, 1095, 1100, 1103, 1108, 1115, 1120, 1134, 1141, 1144, 1155, 1166, 1168, 1179, 1194, 1202, 1208, 1210, 1216, 1235, 1246, 1258, 1269, 1274, 1281, 1295, 1300, 1305, 13...
```

---

### toMap, sampleByKey

```
scala> val randRDD = sc.parallelize(List((7,"cat"), (6,"mouse"), (7,"cup"), (6,"book"), (7,"tv"), (6,"green"), (7,"heater")))
randRDD: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[28] at parallelize at <console>:24

scala> val sampleMap = List((7,0.4),(6,0.6)).toMap
sampleMap: scala.collection.immutable.Map[Int,Double] = Map(7 -> 0.4, 6 -> 0.6)

///sampleByKey - sampleMap에 있는 키 값과 비율에 따라 샘플링
scala> randRDD.sampleByKey(false, sampleMap, 42).collect
res32: Array[(Int, String)] = Array((6,book), (7,tv), (7,heater))
```

---

### distinct

```
scala> val c = sc.parallelize(List("Gnu", "Cat", "Rat", "Dog", "Gnu", "Rat"), 2)
c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[37] at parallelize at <console>:24

scala> c.distinct.collect
res40: Array[String] = Array(Dog, Cat, Gnu, Rat)

scala> val a = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10))
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[41] at parallelize at <console>:24

///partition을 3개로 분할
scala> val b = a.distinct(3).partitions
b: Array[org.apache.spark.Partition] = Array(org.apache.spark.rdd.ShuffledRDDPartition@0, org.apache.spark.rdd.ShuffledRDDPartition@1, org.apache.spark.rdd.ShuffledRDDPartition@2)
```

---

### textFile, first, saveAsTextFile, persist

```
$ pwd
/home/hadoop/practice4_spark_test
```
```
$ vi test.txt
    ---------
    park also comes with several sample programs in the `examples` directory.
    To run one of them, use `./bin/run-example <class> [params]`. For example:

      ./bin/run-example SparkPi

    will run the Pi example locally.
    ---------
```

```
scala> val inputfile =sc.textFile("file:///home/hadoop/practice4_spark_test/test.txt")
inputfile: org.apache.spark.rdd.RDD[String] = file:///home/hadoop/practice4_spark_test/test.txt MapPartitionsRDD[61] at textFile at <console>:24

scala> val counts = inputfile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_+_)
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[64] at reduceByKey at <console>:26

scala> counts.collect
res50: Array[(String, Int)] = Array((example,1), (park,1), (<class>,1), (one,1), (locally.,1), (with,1), (will,1), (several,1), ("",6), ([params]`.,1), (SparkPi,1), (example:,1), (sample,1), (them,,1), (For,1), (run,2), (Pi,1), (./bin/run-example,1), (`./bin/run-example,1), (in,1), (of,1), (also,1), (programs,1), (To,1), (comes,1), (directory.,1), (`examples`,1), (use,1), (the,2))
```
```
scala> inputfile.first
res51: String = "park also comes with several sample programs in the `examples` directory. "

scala> counts.saveAsTextFile("output")

scala> counts.persist
res54: counts.type = ShuffledRDD[64] at reduceByKey at <console>:26
```
```
///터미널 복귀
hadoop@hadoopsvr ~/practice4_spark_test $ hdfs dfs -ls -R output
    ---------
    -rw-r--r-- 2 hadoop supergroup 0 2017-02-03 11:18 output/_SUCCESS
    -rw-r--r-- 2 hadoop supergroup 150 2017-02-03 11:18 output/part-00000
    -rw-r--r-- 2 hadoop supergroup 167 2017-02-03 11:18 output/part-00001
    ---------
    
hadoop@hadoopsvr ~/practice4_spark_test $ hdfs dfs -cat output/part-00000
    ---------
    (example,1)
    (park,1)
    (<class>,1)
    (one,1)
    (locally.,1)
    (with,1)
    (will,1)
    (several,1)
    (,6)
    ([params]`.,1)
    (SparkPi,1)
    (example:,1)
    (sample,1)
    (them,,1)
    ---------
```

---

### zip, keyBy

```
scala> val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[66] at parallelize at <console>:24

scala> val b = a.map(_.length)
b: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[68] at map at <console>:26

scala> val c = a.zip(b)
c: org.apache.spark.rdd.RDD[(String, Int)] = ZippedPartitionsRDD2[69] at zip at <console>:28

scala> c.collect
res56: Array[(String, Int)] = Array((dog,3), (salmon,6), (salmon,6), (rat,3), (elephant,8))

///length를 키로 삼아랴
scala> val d = a.keyBy(_.length)
d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[71] at keyBy at <console>:26

scala> d.collect
res58: Array[(Int, String)] = Array((3,dog), (6,salmon), (6,salmon), (3,rat), (8,elephant))

scala> d.keys.collect
res59: Array[Int] = Array(3, 6, 6, 3, 8)

scala> d.values.collect
res60: Array[String] = Array(dog, salmon, salmon, rat, elephant)
```

---

### foldByKey

```
scala> val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)
a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[74] at parallelize at <console>:24

scala> val b = a.map(x => (x.length, x))
b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[75] at map at <console>:26

scala> b.collect
res61: Array[(Int, String)] = Array((3,dog), (3,cat), (3,owl), (3,gnu), (3,ant))

///키값이 같은 것 끼리 합친다
scala> b.foldByKey(" ")(_ + _).collect
res62: Array[(Int, String)] = Array((3," dogcat owlgnuant"))
```

---

### join, leftOuterJoin

```
scala> val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
a: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[81] at parallelize at <console>:24

scala> val b = a.keyBy(_.length)
b: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[82] at keyBy at <console>:26

scala> val c = sc.parallelize(List("dog", "cat", "gnu", "salmon", "rabbit", "turkey", "wolf", "bear", "bee"), 3)
c: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[83] at parallelize at <console>:24

scala> val d = c.keyBy(_.length)
d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[84] at keyBy at <console>:26

scala> b.join(d).collect
res69: Array[(Int, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)), (6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)), (3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))

scala> b.leftOuterJoin(d)
res70: org.apache.spark.rdd.RDD[(Int, (String, Option[String]))] = MapPartitionsRDD[90] at leftOuterJoin at <console>:33
```

---

### Spark SQL

```
> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
> import sqlContext.implicits._
```

1. SQLContent를 이용하여 위의 데이터를 로딩하시오.
```
> case class hlist(cid:Int, name:String, age:Int)
> val dfhlist = sc.textFile("file:///home/hadoop/practice4_spark_test/homelists.txt").map(_.split(",")).map(p => hlist(p(0).trim.toInt, p(1), p(2).trim.toInt)).toDF()
> dfhlist.show
```

2. 이름만 출력하시오.
```
> dfhlist.select("name").show
```

3. 이름이 23세 이상인 사람만 출력하시오.
```
> dfhlist.filter(dfhlist("age").gt(23)).show
```

4. 나이로 그루핑하고 카운트하시오.
```
> dfhlist.groupBy("age").count.show
```

5. 이름과 나이대로 나누어 20대, 30대로 출력하시오.(20대, 30대로 나누어 정보를 출력)
```
> val age:Int => Int = _ / 10 * 10
> val intUDF = udf(age)
> dfhlist.withColumn("age", intUDF('age)).show
```
