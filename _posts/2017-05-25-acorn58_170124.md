---
layout: post
title: Hive, Sqoop - 17/01/24
category: acorn수업
---

## 증권데이터문제
NASDAQ_daily_prices_A_sample.csv, NASDAQ_dividends_A.csv 다운로드       ///github  

> [daily]  
field명  
exchange, stock_symbol, date,  
stock_price_open, stock_price_high, stock_price_low, stock_price_close,  
stock_volume, stock_price_adj_close  


> [dividends]  
field명  
exchange, stock_symbol, date, dividends  


1. NASDAQ daily data set을 위한 external table을 생성하시오.  
    ```
    hive > CREATE EXTERNAL TABLE daily_ex
      (exchange_1 STRING, stock_symbol STRING, date_1 STRING, stock_price_open FLOAT, 
       stock_price_high FLOAT, stock_price_low FLOAT, stock_price_close FLOAT, 
       stock_volume BIGINT, stock_price_adj_close FLOAT) 
      ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' 
      STORED AS TEXTFILE LOCATION '/user/hadoop/daily_ex';

    hive > LOAD DATA LOCAL INPATH '/home/hadoop/practice3_hive_test/NASDAQ_daily_prices_A_sample.csv' 
      OVERWRITE INTO TABLE daily_ex;
    ```

2. NASDAQ dividends data set(배당금)을 위한 external table을 생성하시오.
    ```
    hive > CREATE EXTERNAL TABLE dividends_ex
    (exchange_2 STRING, stock_symbol STRING, date_2 STRING, dividends FLOAT) 
    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' 
    STORED AS TEXTFILE LOCATION '/user/hadoop/dividends_ex';

    hive > LOAD DATA LOCAL INPATH '/home/hadoop/practice3_hive_test/NASDAQ_dividends_A.csv' 
    OVERWRITE INTO TABLE dividends_ex;
    ```


3. 종가가 5$이상인 종목의 총거래량에 대한 값을 출력하시오.
      ```
      hive > select stock_price_close, sum(stock_volume) 
        from daily_ex 
        where stock_price_close > 5.0;

      hive > select stock_symbol, sum(stock_volume) 
        from daily_ex 
        where stock_price_close >5.0 
        group by stock_symbol;
      ```


4. 각 종목별 역대 최고가를 출력하시오.
      ```
      hive > select stock_symbol, MAX(stock_price_high) 
        from daily_ex 
        GROUP BY stock_symbol;
      ```


5. 역대 종목별 최고가 배당에 대하여 출력하시오.
      ```
      hive > SELECT stock_symbol, MAX(dividends) 
        FROM dividends_ex 
        GROUP BY stock_symbol;
      ```


6. 최고가와 최고배당이 존재한다면 최고가와 최고배당을 출력하시오.
      ```
      hive > select A.stock_symbol, MAX(A.stock_price_high), MAX(B.dividends) 
        from daily_ex A JOIN dividends_ex B on (A.stock_symbol=B.stock_symbol) 
        group by A.stock_symbol;
      ```


7. 최고가와 최고배당을 출력하시오. 만약 그 중 하나가 존재하지 않는다면 null로 두시오(즉, outer join)
      ```
      hive > select A.stock_symbol, MAX(A.stock_price_high), MAX(B.dividends) 
        FROM daily_ex A LEFT OUTER JOIN dividends_ex B on (A.stock_symbol=B.stock_symbol) 
        group by A.stock_symbol;

      hive > select B.stock_symbol, MAX(A.stock_price_high), MAX(B.dividends) 
        FROM dividends_ex B LEFT OUTER JOIN daily_ex A on (A.stock_symbol=B.stock_symbol) 
        group by B.stock_symbol;
      ```

---

## Sqoop

실 서비스 환경에서는  아래와 같은 경우가 있을 것이다.  

- 별도의 로그 수집 시스템 및 데이터 저장소가 마련되지 않아 **Oracle, MySQL 등의 RDBMS에 로그를 저장하는 경우**. 즉, RDBMS로 계속 누적되어 **비대해진 데이터를 분석하기에는 비용과 시간이 많이 소요되어 Hadoop과 같은 분산환경의 저장소로 옮겨 분석할 필요가 발생함**.
- 로그 뿐 아니라, **메타성 데이터는 대부분 RDBMS에 저장**되어 있는데, 이 RDBMS의 **메타 데이터를 Hadoop, Hive 등으로 옮겨야 하는 경우**.
- **반대로, 분산 환경의 Hadoop, Hive 등에서 분석된 결과**를 API형태가 아닌 **원격의 RDBMS로 전송할 경우**가 발생함.

위와 같은 경우 중간 과정없이 손 쉽게 옮길 수 있는 방안을 고민하게 되는데. 이럴때 가장 유용한 오픈 소스가 Apache Sqoop이다.  

Apache Sqoop은 **Sql to Hadoop**의 의미로서, **Apache Hadoop과 RDBMS 사이에 bulk data를 효율적이고 쉽게 옮길 수 있도록 디자인 된 도구**이다.  

[출처](http://hochul.net/blog/datacollector_apache_sqoop_from_rdbms/){:target="_blank"}

---

## Sqoop 설치

```
$ pwd
/home/hadoop/

$ wget http://apache.mirror.cdnetworks.com/sqoop/1.4.6/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz
$ tar -vxzf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz

$ mkdir sqoop
///링크로 생성할폴더는 따로 mkdir로 만들지 않는다
$ln -s sqoop-1.4.6.bin__hadoop-2.0.4-alpha sqoop

$ cp /usr/share/java/mysql-connector-java.jar /home/hadoop/sqoop/lib

$ cd sqoop/conf
$ cp sqoop-site-template.xml sqoop-site.xml
$ cp sqoop-env-template.sh sqoop-env.sh

$ vi sqoop-env.sh   ///github
$ vi ~/.bashrc    ///github
$ source ~/.bashrc
```

```
///메일에서 weblog_entries.txt를 다운로드

///jw사용자로 터미널 접속 -> root 사용자로 이동
$ sudo su
# mkdir /var/lib/mysql/logs -p
# mv /home/jw/Downloads/weblog_entries.txt /var/lib/mysql/logs
# chmod 777 /var/lib/mysql/logs/weblog_entries.txt
# chown -R mysql:mysql /var/lib/mysql/logs/

///root 사용자에서 할 것
$ mysql -u root -p
mysql> show databases
mysql> use logs

mysql> create table weblogs(md5 varchar(32), url varchar(64), request_date DATE, request_time TIME, ip varchar(15)) engine=InnoDB;

mysql> create table weblogs_from_hdfs(md5 varchar(32), url varchar(64), request_date DATE, request_time TIME, ip varchar(15)) engine=InnoDB;

mysql> LOAD DATA LOCAL INFILE '/var/lib/mysql/logs/weblog_entries.txt' INTO TABLE weblogs FIELDS TERMINATED BY '      ' LINES TERMINATED BY '\r\n';

mysql> LOAD DATA LOCAL INFILE '/var/lib/mysql/logs/weblog_entries.txt' INTO TABLE weblogs FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\r\n';
LOAD DATA LOCAL INFILE '/logs/weblog_entries.txt' INTO TABLE weblogs FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\r\n';
```
```
$ sudo cp /usr/share/java/mysql-connector-java.jar $SQOOP_HOME/lib
$ sqoop import -m 1 --connect jdbc:mysql://localhost/logs --username root --password 123 --table weblogs --target-dir /data/weblogs/import

///-m 1 ===> mapreduce할 개수 (container?)
/// --connect jdbc:mysql://localhost/logs ===> logs는 데이터베이스 이름
/// --table weblogs ===? 테이블이름은 weblogs
/// --target-dir /data/weblogs/import ===> hdfs에 올라가는 위치
$ sqoop import -m 1 --driver com.mysql.jdbc.Driver --connect jdbc:mysql://localhost/logs --username root --password 123 --table weblogs --target-dir /data/weblogs/import

$ sqoop export -m 1 --driver com.mysql.jdbc.Driver --connect jdbc:mysql://localhost/logs --username root --password 123 --table weblogs_from_hdfs --export-dir /data/weblogs/import/part-m-00000 --input-fields-terminated-by ',  ' --mysql-delimiters
```
