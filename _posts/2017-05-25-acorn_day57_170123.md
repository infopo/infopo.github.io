---
layout: post
title: Hive - 17/01/23
category: acorn수업
---

## Hive vs MySQL

While each tool performs a similar general action, retrieving data, each does it in a very different way. Whereas **Hive** is *intended as a convenience/interface for querying data stored in HDFS*, **MySQL** is *intended for online operations requiring many reads and writes.*  

One good example of this difference in action is in forming table schemas. **Hive** uses a method of querying data known as *“schema on read”*, which allows a user to redefine tables to match the data without touching the data. Hive has serialization and deserialization adapters to let the user do this, so *it isn’t intended for online tasks requiring heavy read/write traffic*. On the flip side, **MySQL** utilizes *“schema on write”*, which means you *define table schemas before you can add data to the store*. This allows MySQL to store it in an optimal way for *fast reading and writing*. These differing approaches are a good example of how these two technologies differ.

### WHEN TO USE HIVE

- If you have large (think terabytes/petabytes) datasets to query
- If extensibility is important

### WHEN TO USE MYSQL

- If performance is key
- If your datasets are relatively small (gigabytes)
- If you need to update and modify a large number of records frequently

[출처](https://blog.matthewrathbone.com/2015/12/08/hive-vs-mysql.html){:target="_blank"}

---

Below are the key features of Hive that differ from RDBMS.

- Hive resembles a traditional database by supporting SQL interface but it is not a full database. Hive can be better called as data warehouse instead of database.
- Hive enforces schema on read time whereas RDBMS enforces schema on write time.

In RDBMS, a table’s schema is enforced at data load time, If the data being
loaded doesn’t conform to the schema, then it is rejected. This design is called schema on write.

But Hive doesn’t verify the data when it is loaded, but rather when a
it is retrieved. This is called schema on read.

Schema on read makes for a very fast initial load, since the data does not have to be read, parsed, and serialized to disk in the database’s internal format. The load operation is just a file copy or move.

Schema on write makes query time performance faster, since the database can index columns and perform compression on the data but it takes longer to load data into the database.

- Hive is based on the notion of Write once, Read many times but RDBMS is designed for Read and Write many times.
- In RDBMS, record level updates, insertions and deletes, transactions and indexes are possible. Whereas these are not allowed in Hive because Hive was built to operate over HDFS data using MapReduce, where full-table scans are the norm and a table update is achieved by transforming the data into a new table.
- In RDBMS, maximum data size allowed will be in 10’s of Terabytes but whereas Hive can 100’s Petabytes very easily.
- As Hadoop is a batch-oriented system, Hive doesn’t support OLTP (Online Transaction Processing) but it is closer to OLAP (Online Analytical Processing) but not ideal since there is significant latency between issuing a query and receiving a reply, due to the overhead of Mapreduce jobs and due to the size of the data sets Hadoop was designed to serve.
- RDBMS is best suited for dynamic data analysis and where fast responses are expected but Hive is suited for data warehouse applications, where relatively static data is analyzed, fast response times are not required, and when the data is not changing rapidly.
- To overcome the limitations of Hive, HBase is being integrated with Hive to support record level operations and OLAP.
- Hive is very easily scalable at low cost but RDBMS is not that much scalable that too it is very costly scale up.

## Hive 설치

```
$ pwd
/home/hadoop
$ mkdir practice3_hive_test
$ wget http://apache.tt.co.kr/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz
$ tar -vxzf apache-hive-2.1.1-bin.tar.gz
$ ln -s apache-hive-2.1.1-bin hive

$ cd hive
$ cd conf
$ cp hive-default.xml.template hive-default.xml
$ cp hive-env.sh.template hive-env.sh
$ vi hive-site.xml    ///github

$ pwd
/home/hadoop/hive/conf/

$ vi hive-env.sh
///아래를 주석처리 후 나간다
#export HIVE_AUX_JARS_PATH=$HIVE_HOME/udf
```
```
$ mysql
> use mysql
> insert into user (Host, User, authentication_string, ssl_cipher, x509_issuer, x509_subject)
values('%','hive',password('hive'),'','','');
> grant all privileges on *.* to 'hive'@'localhost' identified by 'hive';
> flush privileges;
```
```
$ hive 폴더
$ cd ..
$ cd lib
///이미 설치가 되어 있었음
$ sudo apt-get install libmysql-java

$ dpkg -L libmysql-java
$ sudo cp /usr/share/java/mysql-connector-java-5.1.38.jar /home/hadoop/hive/lib

$ cd ..
$ cd conf
$ vi hive-env.sh    ///github

$ vi ~/.bashrc    ///github

$ source ~/.bashrc
$ schematool -initSchema -dbType mysql
```
```
///mysql로 복귀
> use hive
> show tables
```

---

## Hive 실행

```
$ hive
hive> create database userdb;
hive> show databases;
hive> use userdb;

hive> create table if not exists employee (eid int, name String, salary String, destination String) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

hive> show tables;
hive> ALTER TABLE employee RENAME TO emp;
```
```
$ pwd
/home/hadoop/practice3_hive_test
$ vi emp.txt
$ hdfs dfs -put emp.txt /user/hive/
```
```
///hive (리눅스에 있는 경로로 찾을 때) (hdfs dfs -put 과 상관 없음)
hive> LOAD DATA LOCAL INPATH '/home/hadoop/practice3_hive_test/emp.txt' OVERWRITE INTO TABLE emp;

///hdfs에 있는 파일을 테이블에 읽힐 때(INPATH의 기본 경로는 /user/hadoop 이다.)
hive> LOAD DATA INPATH 'emp.txt' OVERWRITE INTO TABLE emp;
```
```
$ hdfs dfs -ls -R /user/hive
    ---------
    -rw-r--r-- 2 hadoop supergroup 151 2017-01-23 12:26 /user/hive/emp.txt
    drwxr-xr-x - hadoop supergroup 0 2017-01-23 11:56 /user/hive/metastore
    drwxr-xr-x - hadoop supergroup 0 2017-01-23 12:17 /user/hive/metastore/userdb.db
    drwxr-xr-x - hadoop supergroup 0 2017-01-23 12:28 /user/hive/metastore/userdb.db/emp
    -rwxr-xr-x 2 hadoop supergroup 151 2017-01-23 12:28 /user/hive/metastore/userdb.db/emp/emp.txt
    ---------
```
```
///namenode가 readonly일 경우 해결방법
$ hdfs dfsadmin -safemode leave

$ hdfs dfs -lsr /user/hive/
    ---------
    lsr: DEPRECATED: Please use 'ls -R' instead.
    drwxr-xr-x - hadoop supergroup 0 2017-01-23 11:56 /user/hive/metastore
    drwxr-xr-x - hadoop supergroup 0 2017-01-23 11:56 /user/hive/metastore/userdb.db
    ---------
```
```
$ hive
hive> select * from emp sort by eid DESC;
hive> select * from emp order by eid DESC;
hive> insert into emp(eid, name, salary, destination) values(1212, 'kim', 45000, 'tp');
hive> insert into emp values(1213, 'lee', 25000, 'TA');
```

---

## drivers.csv 파일

```
///drivers.csv 다운로드, github
///제목 행은 삭제할 것

/// /practice3_hive_test로 이동
$ sudo mv ~/Downloads/drivers.csv /home/hadoop/practice3_hive_test

$ hive
hive> create table temp_drivers(col_value STRING);
hive> LOAD DATA LOCAL INPATH '/home/hadoop/practice3_hive_test/drivers.csv' OVERWRITE INTO TABLE temp_drivers;
hive> select * from temp_drivers;

hive> CREATE TABLE drivers(driverID int, name String, ssn BIGINT, location String, certified String, wageplan String);

hive> insert overwrite table drivers SELECT
  > regexp_extract(col_value, '^(?:([^,]*),?){1}',1) driverID,
  > regexp_extract(col_value, '^(?:([^,]*),?){2}',1) name,
  > regexp_extract(col_value, '^(?:([^,]*),?){3}',1) ssn,
  > regexp_extract(col_value, '^(?:([^,]*),?){4}',1) location,
  > regexp_extract(col_value, '^(?:([^,]*),?){5}',1) certified,
  > regexp_extract(col_value, '^(?:([^,]*),?){6}',1) wageplan
  > from temp_drivers;

hive> select * from drivers;
```

---

## timesheet.csv 파일

```
///timesheet.csv 다운로드, github
$ sudo mv timesheet.csv /home/hadoop/practice3_hive_test/
hive> create table temp_timesheet(col_value string);
hive> load data local inpath '/home/hadoop/practice3_hive_test/timesheet.csv' overwrite into table temp_timesheet;
hive> CREATE TABLE timesheet(driverID int, week int, hours_logged int, miles_logged int);

hive> insert overwrite table timesheet SELECT regexp_extract(col_value, '^(?:([^,]*),?){1}',1) driverID, regexp_extract(col_value, '^(?:([^,]*),?){2}',1) week, regexp_extract(col_value, '^(?:([^,]*),?){3}',1) hours_logged, regexp_extract(col_value, '^(?:([^,]*),?){4}',1) miles_logged from temp_timesheet;
hive> select * from timesheet;
```

1. timesheet테이블에서 driverID별로 시간과 거리 합을 출력하시오.
  ```
  hive> select driverid, sum(hours_logged), sum(miles_logged) from timesheet group by driverid;
  hive> select sum(B.hours_logged), sum(B.miles_logged) from timesheet B group by driverid;
  ```
  
2. 드라이버 아이디별로 시간과 거리합을 그루핑한 다음 drivers테이블과 driverID로 join하시오.
  ```
  hive> select A.driverid, A.name, A.ssn, A.location, A.certified, A.wageplan, sum(B.hours_logged), sum(B.miles_logged)
    > from drivers A
    > join timesheet B on A.driverid = B.driverid
    > group by A.driverid, A.name, A.ssn, A.location, A.certified, A.wageplan;
  ```

3. partition을 이용하고 키는 date와 country로 한다.  
    ```
    $ vi devicecheck.csv
      ---------
      1,u1,en,iphone,201503210011,http://xxx/xxx/1,20150321,US
      2,u1,en,ipad,201503220111,http://xxx/xxx/2,20150322,US
      3,u2,en,desktop,201503210051,http://xxx/xxx/3,20130321,CA
      4,u2,en,iphone,201503230021,http://xxx/xxx/4,20150323,HK
      ---------
    hive> create table temp_devicecheck(id int, user_id String, user_lang String, user_device String, time_stamp BIGINT, url String, date777 BIGINT, country String) PARTITIONED BY (con_date BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

    hive> LOAD DATA LOCAL INPATH '/home/hadoop/practice3_hive_test/devicecheck.csv' OVERWRITE INTO TABLE temp_devicecheck PARTITION (con_date='20150321');

    hive> select * from temp_devicecheck;
    hive> select * from temp_devicecheck where con_date='20150321';

    hive> create table temp_devicecheck_raw(id int, user_id String, user_lang String, user_device String, time_stamp BIGINT, url String, date777 BIGINT, country String) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE;

    hive> insert overwrite table temp_devicecheck partition (con_date='20150322') select * from temp_devicecheck_raw where date777=20150322;

    hive> show partitions temp_devicecheck;
    ```
