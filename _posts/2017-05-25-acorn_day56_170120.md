---
layout: post
title: Pig - 17/01/20
category: acorn수업
---

## WORDCOUNT

```
$ start-all.sh
$ mr-jobhistory-daemon.sh start historyserver
$ pwd
/home/hadoop
$ cd hadoop

$ hdfs dfs -put README.txt /user/hadoop

$ pig

///line은 사용자가 임의로 정하는 값
grunt> input_lines = LOAD 'README.txt' USING PigStorage() AS (line:chararray);
grunt> dump input_lines

///tokenize(line) 단어별로 구분 -> tuple이나 bag으로 변함(행 단위)
///flatten -> 각각 개별 값으로 바꿔라
grunt> words = FOREACH input_lines GENERATE FLATTEN(TOKENIZE(line)) AS word;

///MATCHES '\\w+' 정규표현식 - 알파벳만 있는 것들만 잡아내라
grunt> filtered_words = FILTER words BY word MATCHES '\\w+';

grunt> word_groups = GROUP filtered_words BY word;
grunt> word_count = FOREACH word_groups GENERATE group AS word, COUNT(filtered_words) AS count;
grunt> ordered_word_count = ORDER word_count BY count DESC;

grunt> STORE ordered_word_count INTO 'README_count';
grunt> cat README_count
```

---

## 영화데이터 이용한 문제

이메일에서 영화 데이터 다운로드 - github

```
$ pwd
/home/jw/Download
$ unzip movie_data.zip
$ sudo mv *.csv /home/hadoop/practice2_pig_test/
$ pwd
/home/hadoop/pratice2_pig_test

$ hdfs dfs -put *.csv /user/hadoop
$ hdfs dfs -ls /user/hadoop
```

1. 필드명: id, name, year, rating, duration으로 로딩하고 메모리 덤프로 확인하고, schema를 확인하시오. (relation명: movies)
  ```
  grunt> movies = LOAD 'movies_data.csv' USING PigStorage(',') AS (id:int, name:chararray, year:int, rating:double, duration:double);
  grunt> dump movies
  grunt> describe movies
  ```

2. 평점 4.0이상인 영화를 출력하시오. 저장파일: movies_than_four
  ```
  grunt> movies_than_four_all = filter movies by rating >= 4.0;
  grunt> movies_than_four = foreach movies_than_four_all generate name, rating;
  grunt> store movies_than_four into 'movies_than_four';
  grunt> ls
  grunt> cat movies_than_four
  grunt> dump movies_than_four;
  ```

3. 1950년과 1960년 사이에 출시된 영화리스트를 출력하시오. 저장파일: movies_list_1950_1960
  ```
  grunt> describe movies
      ---------
      movies: {id: int,name: chararray,year: int,rating: double,duration: double}
      ---------
      
  grunt> asdf = filter movies by year>1950 and year<1960;
  grunt> asdf2 = foreach asdf generate year,name;
  grunt> asdf3 = order asdf2 by name asc;
  grunt> store asdf3 into 'movies_list_1950_1960';
  grunt> cat movies_list_1950_1960;
  ```

4. 알파벳 A로 시작하는 영화 리스트를 출력하시오. relation이름: startswith_a_movies_list
    ```
    ///STARTSWITH는 대문자로 해야함!!
    grunt> startswith_a_movies_list = foreach movies generate name, STARTSWITH(name,'A');
    
    grunt> dump startswith_a_movies_list
    ```

5. 상영시간이 2시간 이상인 영화 목록을 출력하시오.
  ```
  grunt> qwer = filter movies by duration>7200;
  grunt> dump qwer;
  ```
  
---

## PIG SCRIPT
```
$pwd
/home/hadoop/practice2_pig_test
$ vi README_count.pig   ///github

$ pig -x mapreduce README_count.pig
$ hdfs dfs -ls /user/hadoop/
hdfs dfs -ls를 해도 동일경로를 찾는다
$ hdfs dfs -cat /user/hadoop/README-count2/part-r-00000
```

---

## JAVA로 PIG 컴파일하기
```
$pwd
/home/hadoop/practice2_pig_test

$ mkdir pig_UDF

$pwd
/home/hadoop/practice1_hadoop_test
$cp hadoop-core-1.2.1.jar ../practice2_pig_test/pig_UDF

$ pwd
/home/hadoop/pig
$ cp pig-0.17.0-SNAPSHOT-core-h2.jar ../practice2_pig_test/pig_UDF/

$ pwd
/home/hadoop/practice2_pig_test/pig_UDF
$ ls
hadoop-core-1.2.1.jar pig-0.17.0-SNAPSHOT-core-h2.jar

$ vi Sample_Eval.java   ///github
///:뒤에 .은 현재 디렉토리를 의미 //// 없어도 되지만 붙이는 습관을 들이도록 할것
$ javac -cp hadoop-core-1.2.1.jar:pig-0.17.0-SNAPSHOT-core-h2.jar:. *.java

$ jar -cvf Sample_Eval.jar Sample_Eval.class
$ cp Sample_Eval.jar ../
$ cd ..

$ pwd
/home/hadoop/practice2_pig_test/
$ hdfs dfs -put Sample_Eval.jar /user/hadoop    ///필요없음
///jar파일을 올린 위치에서 pig를 실행할 것!!!!
///pig 명령어를 치는 위치(리눅스 터미널)에 올리려는 .jar파일이 존재해야 한다.
///REGISTER는 리눅스 터미널의 경로에서 .jar파일을 찾는다! hdfs dfs -put은 필요 없음

$ pig
grunt> pwd
hdfs://192.168.0.91:9000/user/hadoop

grunt> REGISTER './Sample_Eval.jar'
grunt> DEFINE Sample_Eval Sample_Eval();
grunt> student = LOAD 'pig_data/student_data.txt' USING PigStorage(',') AS (id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray);

grunt> Upper_firstname_case = FOREACH student GENERATE Sample_Eval(firstname);
grunt> dump Upper_firstname_case
```

---

## PIG - MONGO DB CONNECTION

```
$ pwd
/home/hadoop/
$ mkdir mongo_pig
$ cd mongo_pig
$ git clone https://github.com/mongodb/mongo-hadoop.git
$ cd mongo-hadoop
$ ./gradlew jar

hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/build/libs $ cd /home/hadoop/hadoop/lib
hadoop@hadoopsvr ~/hadoop/lib $ mkdir mongo-hadoop

///3개의 jar파일을 (/home/hadoop/hadoop/lib/mongo-hadoop)한 곳에 모아야 함

hadoop@hadoopsvr ~ $ cd mongo_pig/mongo-hadoop/build/libs
hadoop@hadoopsvr ~ $ pwd
/home/hadoop/mongo_pig/mongo-hadoop/build/libs
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/build/libs $ cp mongo-hadoop-2.0.1.jar ../../

hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/build/libs $ cd ../../
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop $ cd core/build/libs
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/core/build/libs $ ls
mongo-hadoop-core-2.0.1.jar
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/core/build/libs $ cp mongo-hadoop-core-2.0.1.jar ../../../

hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/core/build/libs $ cd ../../../
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop $ cd pig/build/libs
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/pig/build/libs $ ls
mongo-hadoop-pig-2.0.1.jar
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/pig/build/libs $ cp mongo-hadoop-pig-2.0.1.jar ../../../

////home/hadoop/mongo_pig/mongo-hadoop에 jar파일 3개가 다 모여야 한다
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop/pig/build/libs $ cd ../../../
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop $ pwd
/home/hadoop/mongo_pig/mongo-hadoop

hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop $ cp *.jar /home/hadoop/hadoop/lib/mongo-hadoop/
hadoop@hadoopsvr ~/mongo_pig/mongo-hadoop $ cd ~/hadoop/lib/mongo-hadoop/
hadoop@hadoopsvr ~/hadoop/lib/mongo-hadoop $ ls
mongo-hadoop-2.0.1.jar mongo-hadoop-core-2.0.1.jar mongo-hadoop-pig-2.0.1.jar

REGISTER는 PIG실행시마다 해야 한다.
$ pig
grunt> REGISTER /home/hadoop/hadoop/lib/mongo-hadoop/mongo-hadoop-2.0.1.jar
grunt> REGISTER /home/hadoop/hadoop/lib/mongo-hadoop/mongo-hadoop-core-2.0.1.jar
grunt> REGISTER /home/hadoop/hadoop/lib/mongo-hadoop/mongo-hadoop-pig-2.0.1.jar
grunt> cd pig_data

grunt> student = LOAD 'student_data.txt' USING PigStorage(',') AS (id:int, firstname:chararray, lastname:chararray, phone:chararray, city:chararray);

grunt> dump student
grunt> STORE student INTO 'mongodb://localhost/student.student_data' USING com.mongodb.hadoop.pig.MongoStorage();
```

```
///jw사용자에서 mongo
$ mongo
> show dbs
> use student
> show tables
> db.student_data.find().pretty()
```
```
grunt> student_mongo = LOAD 'mongodb://localhost/student.student_data' USING com.mongodb.hadoop.pig.MongoLoader();
grunt> dump student_mongo
grunt> store student_mongo into 'data_import_from_mongo';
grunt> cat data_import_from_mongo
```
